# Meu Aprendizado

## Índice

- [Hadoop e MapReduce](#hadoop-e-mapreduce)
- [Spark com Pyspark](#spark-com-pyspark)

## Hadoop e MapReduce

O Hadoop foi minha porta de entrada para o mundo do Big Data. Comecei compreendendo a crescente importância dos dados em nossas vidas e como o Hadoop se destaca como uma solução crucial para gerenciar e analisar esses volumes massivos de informações. Explorei a instalação do Hadoop em meu ambiente local, familiarizando-me com os modos standalone e pseudo-distribuído, que me permitiram testar e simular cenários de cluster em uma única máquina. Em seguida, mergulhei no MapReduce, uma abordagem simplificada e poderosa para processamento paralelo de dados. Ao entender o fluxo de trabalho do MapReduce, desde o mapeamento até a redução, ganhei insights valiosos sobre como os dados são processados em etapas para gerar resultados significativos. Além disso, aprofundei-me no conceito de "Job", compreendendo seu papel na coordenação das tarefas MapReduce e na execução eficiente de operações em larga escala.

## Spark com Pyspark

á com o PySpark, dei um passo adiante na minha jornada de aprendizado, explorando uma abordagem mais flexível e poderosa para lidar com Big Data. Comecei com uma sólida introdução aos conceitos fundamentais do Spark e sua arquitetura, entendendo como ele se diferencia em relação ao Hadoop. Segui as instruções detalhadas para configurar meu ambiente de desenvolvimento, incluindo a instalação do Spark em uma máquina virtual Ubuntu. Compreendi os diferentes tipos de dados suportados pelo Spark, como RDDs, DataFrames e Datasets, e aprendi a realizar operações complexas em conjuntos de dados distribuídos. Ao mergulhar no Spark SQL, ganhei habilidades em executar consultas sofisticadas em grandes conjuntos de dados, gerenciando bancos de dados e tabelas de forma eficiente. Desenvolvi aplicações práticas utilizando o PySpark, escrevendo código para processar dados em tempo real e implementar funcionalidades avançadas, como manipulação de parâmetros de entrada e conversão de formatos de arquivo.

Além disso, aprendi a otimizar o desempenho das minhas aplicações Spark, utilizando técnicas como particionamento de dados, cache e persistência de dados. Explorei tópicos avançados, como integração com Notebooks Jupyter e conversão de dados do Pandas para DataFrames do Spark, expandindo ainda mais minhas habilidades analíticas e de desenvolvimento. No geral, essa jornada de aprendizado proporcionou-me uma compreensão profunda e prática de como lidar com Big Data de forma eficiente e escalável, capacitando-me a enfrentar desafios complexos e extrair insights valiosos a partir de grandes volumes de dados.

